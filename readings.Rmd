---
title: "Readings"
output: html_document
---

Last Updated: `r Sys.Date()`

<style>
.title{
display: none;
}
</style>

```{css, echo=FALSE}

.contact-box {
position: relative;
border: 2px solid lightgrey;
padding: 20px;
margin-bottom: 20px;
border-radius: 10px;
background-color: #f9f9f9;
display: block;
width: 100%;
}

.contact-wrapper {
display: flex;
}

.contact-text {
flex-grow: 1;
}

.year {
position: absolute;
top: 10px;
right: 10px;
font-weight: bold;
padding-left: 10px; /* Add left padding to separate the year from the text */
}

h2 {
margin-bottom: 10px;
font-size: 1.3em;
margin-right: 50px; /* Add right margin to separate the title from the year */
}

p {
margin-bottom: 0;
}

/* Custom CSS for the table of contents */

.tocify-item {
display: inline-block;
text-indent: -10px;
padding-left: 20px;
}

.tocify-header {
display: inline-block;
text-indent: 10px;
padding-left: 100px;
background-color: transparent;
}

.tocify-header > .tocify-subheader {
display: inline-block;
text-indent: 0px;
padding-left: 15px; /* Increase the left padding for subheaders to create an additional indent */
}

.list-group-item.highlight {
background-color: rgba(0,0,0,0);
border-color: rgba(0,0,0,0);
}

/* Add hover effect for the TOC items */

.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover{
z-index: 2;
background-color: rgba(0,0,0,0);
border-color: rgba(0,0,0,0);
}



.list-group-item.active{
z-index: 2;
background-color: rgba(0,0,0,0);
border-color: rgba(0,0,0,0);


}

```

This page will house readings & reflections

My boss Andrew Vickers wants me to read 2 papers from our group each day. I will keep a record of my readings and reflections here. All articles obtained by searching my teammates' names on Google Scholar or NCBI (Andrew Vickers, Melissa Assel, Amy Tin, Emily Vertosick, Dan Sjoberg).

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Underpowering in randomized trials reporting a sample size calculation</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/12954462/" class="link">Link to the article</a>
<span class="year">06/23/23</span>
<p> 

</p>
</div>
</div>


<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Parametric versus non-parametric statistics in the analysis of randomized trials with non-normally distributed data</strong></h2>
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1310536/" class="link">Link to the article</a>
<span class="year">06/23/23</span>
<p> 

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Guidelines for Reporting of Figures and Tables for Clinical Research in Urology</strong></h2>
<a href="https://www.auajournals.org/doi/10.1097/JU.0000000000001096" class="link">Link to the article</a>
<span class="year">06/22/23</span>
<p> 

<strong>Vocab</strong>:

<strong>Funnel Plot</strong>: scatterplot of treatment effect against study precision

<br>

Only include figures if they importantly improve the reader's ability to understand the study findings. In my case in the Testis project, make sure included Kaplan-Meier plots serve a purpose

<br>

When including figures/plots, make sure that they give an immediate visual impression that cannot be captured similary in some quick words

<br>

Good Kaplan-Meier advice: truncate follow-up time for KM figs when # at risk <5 (or even 10)

<br>

<strong>Interesting</strong>: Avoid repeating in the text the results reported in the tables. Refer to all tables in the text by providing a general summary or overall inter-pretation. <strong>This is a little tricky because you have to know what the literature says to make the interpretations</strong>

<br>

This suggestion is directly relevant to my work on the testis project, since I had to use 'add_global_p' to get the overall p-value for the column, and not the individual predictors, in the multivariable LR tables: For categorical variables, a single hypothesistest is preferable to multiple tests at eachlevel of the category.

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Writing Up Clinical Research: A Statistician’s View</strong></h2>
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3744309/" class="link">Link to the article</a>
<span class="year">06/22/23</span>
<p> 

<strong>Notes</strong>:

A good guideline: Don't mention things that are unfalsifiable, as they hold no value (e.g. “surgeons should carefully consider the extent of surgery”)

<br>

Regarding statistical methods, each method should state specifically what it is testing

<br>

Literature review in discussion should only pertain to the the findings and that's it

<br>

Don't just mention study limitations, but also explain the effects of the study limitations. In this paper Andrew mentions: Discussion of limitations should include both the likelihood and effect size of possible bias.

<br>

Just because you reject the null hypothesis doesn't mean it should lead to clinical recommendations

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Defining the Impact of Family History on Detection of High Grade Prostate Cancer in a Large Multi-institutional Cohort</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/34980493/" class="link">Link to the article</a>
<span class="year">06/21/23</span>
<p> 

<strong>Notes</strong>:

<br>

Only read up to end of results

<br>

The researchers used a statistical method called "multiple imputation with chained equations" to fill in missing data for certain variables, and then repeated their main analyses with this filled-in data to ensure their results were not significantly influenced by these missing values.

<br>

Sensitivity analyses using multiple imputation seems to be a common thing when evaluating how missingness affects analyses

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Impact of an Expanded Definition of Family History on Outcomes of Active Surveillance for Prostate Cancer</strong></h2>
<a href="https://www.auajournals.org/doi/10.1097/JU.0000000000003396" class="link">Link to the article</a>
<span class="year">06/21/23</span>
<p> 

<strong>Notes</strong>:

<br>

I only read up to end of results

<br>

Point of paper is to outline that family history for PCa is a risk factor for PCa, but that family history including history of any other malignancies suggestive of hereditary cancer syndrome can also be a risk factor

<br>

Appears this paper's results section is just repeating what's in the tables

<br>

<strong>Observer Bias</strong>: Refers to the distortion in research results that occurs when an experimenter's expectations or preconceptions unconsciously influence the collection, interpretation, or analysis of data.

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>A commentary on PSA velocity and doubling time for clinical decisions in prostate cancer</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/24581521/" class="link">Link to the article</a>
<span class="year">06/20/23</span>
<p> 

<strong>Notes</strong>:

<br>

Every sentence seems to be saying something unequivocally, and the ideas flow from one to the next, need to include this in my writing.

<br>

How to evaluate a marker: (1) Marker must be significantly different between control and disease group, (2) Marker must add information not previously known

<br>

This sentence makes sense, but I want to make it click instantaneously: Several groups have reported that PSA velocity fails to improve the specificity of PSA for biopsy, that is, it does not help the decision to biopsy men with elevated PSA.19-23

<br>

The sentence above basically says that PSAV fails to improve specificity for PSA for biopsy (doesn't help decision to biopsy). Basically, it doesn't maximize the TP/(TP + FP). The FPs are still left as is or worse.

<br>

<strong>Ask Andrew about this sentence</strong>: PSA velocity was not found to improve the sensitivity of PSA: using high PSA velocity as a criterion to biopsy men with low PSA would lead to many millions of additional biopsies per year, without a correspondingly important increase in the number of high-grade cancers detected. 

<strong>Regarding the above</strong>, it says PSAV not found to increase sensitivity, but then it says it would lead to additional biopsies without correspondingly important increase in # high-grade cancers detected. The first part and second part don't match, since the second part implies that sensitivity is decreased. If that is the case, why not just say that PSAV decreases sensitivity? If that isn't the case, then the second part is wrong.

<strong>Andrew's response</strong>: Additional biopsies have nothing to do with the sensitivity since sensitivity only cares about total # of prostate cancer, 

<br>

How does use of randomized trial data avoid these issues: In clinical co- horts, it might not be clear how a patient was referred, whether the PSA record is complete, or what the criteria for biopsy were, leading to problems such as verification bias. Use of randomized trial data avoids these problems.

<br>

Answer to above: Randomized trials, through their design, control for factors such as referral methods, biopsy criteria, and ensure complete record-keeping, thereby reducing biases common in clinical cohorts.

<br>

<strong>PSA Velocity should not be used to decide whether or not to do a biopsy</strong>, as it will lead to many unnecessary procedures. However, it can aid patient counselling in advanced disease stages. PSA changes (high velocity) while on treatment for advanced diease can indicate disease resistance to treatment.

<br>

<strong>Vocab</strong>

<br>

<strong>PSA Velocity</strong>: PSA velocity is a term used to describe the rate of change in PSA levels over time. Specifically, it's a measure of how rapidly PSA levels increase within a specific period. 

<br>

<strong>Receiver Operating Characteristic Curve (ROC)</strong>: plotting TPR vs FPR for single classifier at a variety of thresholds

<br>

<strong>risk count method for calculating PSA</strong>: Can't find this definition anywhere

<br>

<strong>verification bias</strong>: occurs during investigations of diagnostic test accuracy when there is a difference in testing strategy between groups of individuals, leading to differing ways of verifying the disease of interest

<br>

<strong>Standard Arms</strong>: In the context of clinical trials, "standard arms" refers to the control groups that receive the currently accepted and widely used treatment for a condition.

<br>

<strong>metastatic hormone-refractory prostate cancer</strong>: also known as castration-resistant prostate cancer (CRPC), is a form of prostate cancer that continues to progress despite the use of hormone therapy.

<br>

<strong>concordance index</strong>: a measure used in statistics to assess the predictive accuracy of a model, especially in the context of survival analysis or in scenarios where outcomes are ordered (such as risk scores). It is a generalization of the area under the Receiver Operating Characteristic (ROC) curve, which is a graphical plot that illustrates the diagnostic ability of a binary classifier system. 

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>The Four-Kallikrein Panel Is Effective in Identifying Aggressive Prostate Cancer in a Multiethnic Population</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/32385116/" class="link">Link to the article</a>
<span class="year">06/19/23</span>
<p> 

<strong>Notes</strong>:

<br>

4k score better at discrimination among those with higher PSAs

<br>

Not too many figures/tables, and didn't use DCA. Wouldn't this have been an ideal place to use it? Could compare all different models (4k, 4k + Polygenic Risk Score (PRS), PSA, fPSA, etc)

<br>

<strong>Vocab</strong>

<br>

<strong>Nested case-control study</strong>: In a nested case-control study, from a big group, you compare people who got a disease ("cases") with similar people who didn't ("controls") to find differences that might explain why the disease occurred.

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Comparison between the four-kallikrein panel and Prostate Health Index (PHI) for predicting prostate cancer</strong></h2>
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4503229/" class="link">Link to the article</a>
<span class="year">06/19/23</span>
<p> 

<br>

<strong>Notes</strong>

<br>

I never knew measuring total PSA produced an excess number of false positives

<br>

Just defining sensitivity and specificity below since I always forget their defns and which one is which

<br>

re this line in paper: 'High-grade cancer was defined as Gleason score ≥7', this is gleason score of 3+4 or higher, but there is debate around 3+4 being high grade. 4+3, however, is definitely high-grade

<br>

re this line: Briefly, decision curve analysis graphically illustrates the net benefit obtained by using the predictive models in a patient by assuming that the threshold probability for having all prostate cancer or highgrade prostate cancer at which a patient would opt for biopsy is informative of how the patient weighs the relative harms of a false-positive and a <strong>false-negative prediction</strong>. <strong>How does DCA take into account false negatives?</strong>

<br>

In this paper, figures/tables at the end. Seems w.r.t. format there's leeway.

<br>

<strong>Vocab</strong>

<br>

<strong>Sensitivity</strong>: Sensitivity (also called the true positive rate, the recall, or probability of detection in some fields) measures the proportion of actual positives that are correctly identified as such. In other words, it's a measure of how well a test correctly identifies a condition when that condition is indeed present.

<br>

<strong>Specificity</strong>: Specificity (also called the true negative rate) measures the proportion of actual negatives that are correctly identified as such. That is, it's a measure of how well a test identifies the absence of a condition.

<br>

<strong>discrimination</strong>: This is the ability of a predictive model to differentiate between positive cases and negative cases. In other words, it's a measure of how well the model can distinguish or 'discriminate' between those who have an event or condition and those who do not. A commonly used statistic for measuring discrimination is the area under the receiver operating characteristic curve (AUC-ROC). A perfect model has a discrimination of 1, while a model that performs no better than random chance has a discrimination of 0.5 when using AUC-ROC.

<br>

<strong>calibration</strong>: This refers to the agreement between predicted probabilities of an event and the observed frequencies of the event. A model is said to be well-calibrated if the probabilities it predicts correspond well with the actual outcome frequencies. For example, among patients who are all predicted to have a 30% chance of a disease, about 30% of them should actually have the disease for the model to be well-calibrated. Calibration can be assessed visually using a calibration plot or more formally using statistics like the Brier score or the Hosmer-Lemeshow test.





</p>
</div>
</div>


<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Guidelines for reporting of statistics in European Urology</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/25037638/" class="link">Link to the article</a>
<span class="year">06/16/23</span>
<p> 

<br>

<strong>Notes</strong>

<br>

This is basically the older version of Melissa's guidelines. Still skimmed through this.

<br>

<strong>Interesting: Avoid reporting sensitivity and specificity for continuous predictors or a model</strong>

<br>

Basically what I'm getting from above and guidelines in Melissa's paper is that if you can help it, do not categorize continuous variables, and avoid reporting sensitivity/specificity for them

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Guidelines for Reporting of Statistics for Clinical Research in Urology</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/30633111/" class="link">Link to the article</a>
<span class="year">06/16/23</span>
<p> 

<br>

I read this already, but just reading again to let these guidelines sink in.

<br>

<strong>Notes</strong>

<br>

Comparing this to other papers I've read, it seems that there are many different kinds of papers. There are research papers that report data, analyses conducted, results, and conclusions. Then there are papers that report guidelines like this one. Given the breadth of paper paradigms, I wonder what kind of papers are not allowed. 

<br>

This kind of seems like an opinion piece with suggestions.

<br>

2005 Scales et al. paper reports in the month that papers were analyzed, 71% of papers with comparative stats had at least 1 statistical flaw. Now I have to look into this paper, but I wonder what kind of flaws this paper is referring to. Would including an extra sig. fig. for a result in a table constitute an error? If so, I think saying 71% of papers contain at least 1 flaw is disingenuous.

<br>

This Scales et al. paper should be next on my list of papers to read.

<br>

I hate how pedantic statisticians can be about stats. One thing my current boss did in my initial interview with him was ask me to define a p-value. I got it wrong of course, because the true definition is 'the probability of obtaining observed data assuming that the null hypothesis is true.'

<br>

<strong>Guidelines to read</strong>:

<br>

1. CONSORT for randomized trials
2. ReMARK for marker studies
3. TRIPOD for prediction models
4. STROBE for observational studies
5. AMSTAR for systematic reviews

<br>

<strong>Guideline 3.5 is interesting</strong>: the more p-values from testings you have, the commensurately higher the probability that you falsely rejected at least one of them. 

<br>

This carries with it the implicit recommendation that you should prioritize on # of questions, and for each question use only 1 p-value (guideline 3.6/7)

<br>

One thing I need to get into the habit of doing is this: when reporting p-values, be clear about which test the p-value is testing.

<br>

<strong>Typo</strong>: For instance, outcomes can be <strong>compared either side of several different cut-points</strong>, and the optimal cut-point chosen as the one associated with the smallest p-value. 

<br>

<strong>Read up to and including 4.14. Too many guidelines, will come back to this later.</strong>

<br>

<strong>Vocab</strong>

<br>

<strong>umbrella trials</strong>: clin trial to test how well a new substances works on patients with single type of cancer, but different mutations/biomarkers

<br>

<strong>treatment allocation</strong>:  putting patients in different treatment groups in a clinical trial

<br>

<strong>interaction term</strong>: looking at how two or more things affect something together, not just individually

<br>



</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>On the Need for Landmark Analysis or Time-dependent Covariates</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/37037013/" class="link">Link to the article</a>
<span class="year">06/15/23</span>
<p> 

<strong>Notes:</strong> <Notes>

<br>

One of most common/serious stat errors: ignoring time dependency of covariates in survival analyses (guideline 4.15)

<br>

Common survival analysis techniques, such as Cox regression or the Kaplan-Meier method, assume that all covariates are known at the start of follow-up

<br>

guarantee-time bias” (also known as immortal-time bias)

<br>

landmark analysis technique is appropriate when the covariate of interest is known within a relatively short period of time after the start of follow-up

<br>

when performing a landmark analysis it is essential to report the number of subjects excluded in each group because they experienced the event of inter- est or were censored before the landmark, and the number of subjects whose covariate status changed after the landmark. 

<br>

use of time-dependent covariates in Cox regression may be more appropriate than the landmark analysis approach when the value of a co- variate changes during follow-up, where there is not an obvious landmark time, or when the use of a landmark time would lead to an unacceptable number of exclu- sions

<br>

Analysis of a time-dependent covariate in a Cox regression model requires setup of a special- ized data set in a format known as counting process format, in which patients may have multiple rows of data corresponding to different time intervals.

<br>

If the changing value of a covariate over time is ignored, it is likely that the proportional hazards assumption, upon which Cox regression relies, is violated, which can lead to incorrect estimates and conclusions. 

<br>

An important drawback of the time-dependent covariate approach is that it cannot be used to generate prediction of survival over time, and interpretation of the time- dependent coefficients can be difficult.6,7

<br>

Take Home points:

<br>

The use of traditional survival analysis methods such as Cox regression or Kaplan-Meier analysis require
that all covariates are known at the start of follow-up and remain constant throughout the follow-up period.
- Covariates whose status changes during follow-up are known as time-dependent covariates. When the time-de- pendency of covariates is ignored Kaplan-Meier esti- mates are subject to “guarantee-time bias” and the proportional hazards assumption of Cox regression may be violated.
- Cox regression can accommodate a time-varying co- variate using a data set in the appropriate specialized format. But this method cannot be used to generate sur- vival estimates.
- Landmark analysis is useful when the covariate of in- terest is known within a relatively short period of time after the start of follow-up. A fixed landmark time is selected and the start of follow-up and covariates status is defined at the time of the landmark. This method can be used to generate survival estimates, but requires exclusion of patients who had the event or were censored prior to the landmark time.
- Analyses that do not appropriately handle time- dependent covariates violate the assumptions of the standard survival analyses and submissions that do not analyze time-dependent covariates appropri- ately will not pass statistical review at The Journal of Urology!.


</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Innovations in Statistical Review at European Urology</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/30327270/" class="link">Link to the article</a>
<span class="year">06/15/23</span>
<p> 

<strong>Notes:</strong> <Notes>

<br>

<strong>3 principles</strong>:

<br>

(1) we would develop systematic guidance for authors based on a review of common errors in urology research

<br>

(2) all papers with substantive statistics that were under serious consideration for publication in European Urology would be reviewed by a statistician

<br>

(3) we would continue to innovate with respect to statistical reporting.

<br>

The title is misleading because the paper is mostly about things other than 'innovations'. Even the 'innovation' they speak of is just looking at code. Nothing really innovative going on.

</p>
</div>
</div>


<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Intraoperative Ketorolac Associated with Risk of Reoperation After Mastectomy</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/33629252/" class="link">Link to the article</a>
<span class="year">06/14/23</span>
<p> 

<br>

<strong>Notes:</strong> <Notes>

<br>

Why are they using an acronym for enhanced recovery after surgery (ERAS), seems stupid

<br>

Does this REALLY need an acronym?: The postanesthesia care unit (PACU) 

<br>

When they say '...with a multivariable logistic regression model, adjusting for age, BMI,...',
how do you adjust code-wise?

<br>

Again, exclusions in results, guess this is standard practice

<br>

This table does not seem to be generated using gtsummary, there are cells (maybe this is an option? )

<br>

<strong>Vocab</strong>

<br>

<strong>perioperative</strong>: Perioperative refers to the entire span of surgical management, including the preoperative (before surgery), intraoperative (during surgery), and postoperative (after surgery) phases.

<br>

<strong>Ketorolac</strong>: a non-steroidal anti-inflammatory drug (NSAID) commonly used in Enhanced Recovery After Surgery (ERAS) protocols, inhibits cyclooxygenase, which may lead to slower bone healing, compromised postoperative kidney function, and increased perioperative bleeding.

<br>

<strong>lumpectomy</strong>: lump removed from breast

<br>

<strong>contralateral mastectomy</strong>: A contralateral mastectomy is a preventive surgery to remove the breast not affected by cancer to reduce the risk of future breast cancer.

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>PSA concentration at age 60 and death/metastasis from PCa: case-control study</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/20843935/" class="link">Link to the article</a>
<span class="year">06/14/23</span>
<p> 
<br>
<strong>Notes:</strong> <Notes>

<br>

What is 'matching nested' and what is the ratio: Case-control study with 1:3 matching nested

<br>


What is the Vickers Team's affiliation with Sweden? Seems like Emily does a lot of work with Sweden too.

<br>

They use LOWESS for non-linear relation between PSA and outcome, could a GAM be used instead?

<br>

This exclusion, shouldn't it be in the methods and not the results?: We excluded one case (a man who died from prostate cancer)... 

<br>

<strong>Vocab</strong>

<br>

<strong>operating characteristics</strong>: Operating characteristics of a medical test refer to its sensitivity, specificity, and predictive values, which collectively indicate its ability to correctly identify those with and without the disease (GPT definition)

<br>

<strong>nested case-control design</strong>: A nested case-control study is a type of observational research within a larger cohort study that compares exposure to certain risk factors between those who develop an outcome of interest ("cases") and a subset of participants who did not develop the outcome ("controls").

<br>

<strong>labile</strong>: easily altered

<br>

<strong>conditional logistic regression</strong>: Conditional logistic regression is a statistical method used to examine the relationship between a binary outcome and one or more independent variables, while accounting for matching in case-control studies, like testing the association between prostate specific antigen concentrations and the occurrence of a certain condition in a prostate health study.

<br>

<strong>matching in case-control studies</strong>: Matching in case-control studies is a process where each case (individual with a condition) is paired with one or more controls (individuals without the condition) who share similar characteristics, such as age or sex, to help isolate the effect of the exposure being studied.

<br>

<strong>Lorenz Curve</strong>: In this study, the Lorenz curve is used to visually represent the distribution of prostate cancer risk across different prostate specific antigen concentrations, providing insight into how much of the total estimated risk is accounted for by different segments of the population.

<br>




</p>
</div>
</div>


<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>A Contemporary Prostate Cancer Grading System: A Validated Alternative to the Gleason Score</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/26166626/" class="link">Link to the article</a>
<span class="year">06/13/23</span>
<p> 

<br>

<strong>Notes:</strong> <Notes>

<br>

Thought the whole point of CoxPH is to use multiple predictors, but here we have 'Separate univariable and multivariable Cox proportional hazards used four possible categorizations of Gleason scores.' Need to look into this.

<br>

What is the difference between 3+4 & 4+3? A Gleason score 7 can represent mostly well-differentiated cancer with a lesser component of more poorly differentiated cancer (Gleason 3 + 4 = 7) or mostly poorly differentiated cancer with a smaller component of well-differentiated cancer (4 + 3 = 7).

<br>

Interesting that there are no commas in numbers above 999 (e.g. 12 345)

<br>

Typically, BCR defined as post-op PSA value ≥0.2 ng/ml

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Blood prostate specific antigen levels at age 60</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/24682399/" class="link">Link to the article</a>
<span class="year">06/13/23</span>
<p> 

<br>

<strong>Notes:</strong> <Notes>

<br>

Interesting Fact: PSA test was introduced in Sweden in the mid-90s

<br>

EDTA: Ethylenediaminetetraacetic Acid: anticoagulating agent for blood and used to remove heavy metals from blood/body

<br>

This line, have to read up on the reference for it, would like to understand how the PSA values were imputed: 'Owing to the case-control design of the Malmö Preventive Project (three controls matched to each index case), blood was not sampled for some participants and we had to impute their PSA values, using methods previously described'

<br>

This line, have to look into how Nelson-Aalen is used to plot: 'The Nelson-Aalen method was used to plot the cumulative hazards for these events.'

<br>

Why do binomial methods allow for obtaining confidence intervals for risk differences: 'To obtain confidence intervals for risk differences, we used binomial methods.'

<br>

Need to really understand this line, not making sense right now: 'Since there was greater censoring before 15 years in the Gothenburg arm, and because event rates were positively correlated with PSA levels, binomial methods bias against our hypothesis that the benefits of screening are small for men with lower PSA levels.'

<br>

This makes sense, but need it laid out mathematically for more clarity, and also the subsequent calculation for the # needed to be diagnosed: 'We calculated the number needed to screen as the inverse of the absolute risk reduction between the screened and unscreened groups based on 15 years of follow-up.'

<br>

This paper also has results interspersed with figures/tables

<br>

Only read up till discussion (including M&R), did not read discussion

</p>
</div>
</div>


<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>International Prostate Symptom Score Questionnaire</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/37213479/" class="link">Link to the article</a>
<span class="year">06/12/23</span>
<p> 

<br>

<strong>Notes:</strong> <Notes>

<br>

One thing I noted was that the tables and figures are mixed in with the results section, which I did not do for my first actual project. In my project I grouped the tables and figures at the bottom.

Additionally, the tables and figures have shortner names that are more to-the-point than mine, and they don't have repeated information such as the same exclusion criteria listed among all the tables/figures.

<br>

I noticed that, again, the methods and results all flow together to paint a picture that answers the initial research question unequivocally, which is what I need to do in my papers.

<br>

<strong>Criticisms</strong>:

<br>

1. Tables 1&2, variables should be more distinct, and subcategories should be indented further
2. 'The prevalence of discordance was highest for incomplete emptying (24%)' - I don't see this 24% anywhere in tables

</p>
</div>
</div>

<div class="contact-wrapper contact-box">
<div class="contact-text">
<h2><strong>Cancer Label Impact on Low Grade Prostate Cancer</strong></h2>
<a href="https://pubmed.ncbi.nlm.nih.gov/37285311/" class="link">Link to the article</a>
<span class="year">06/12/23</span>
<p> 
<strong>Notes:</strong> <Notes>
<br>
Abstract Parts: 
<br>
1. Background
<br>
2. Objective
<br>
3. Design Setting and Participants
<br>
4. Outcome measurements and statitical analysis
<br>
5. Results and limitations
<br>
6. Conclusions

<br>

Key Words I didn't know:

<strong>discrete choice experiments</strong> (DCE), <strong>conditional logit models</strong>, <strong>marginal rates of substitution</strong> (MRS)

<br>

<strong>Discrete Choice Experiments</strong>: A way of quantifying preferences through analyzing decisions made by participants. Participants pick between competing scenarios with different combinations of attributes from several categorical features.

<br>

<strong>Conditional Logit Models</strong>: A conditional logit model is a statistical model used to examine and model the relationship between a categorical dependent variable and one or more independent variables. This is a specific type of logit model that is particularly useful in cases where the dependent variable represents a choice, and the aim is to understand how different attributes of the choices influence the likelihood of each being selected.

<br>

The materials and methods has 4 sections: <strong>Expert Panel</strong>, <strong>DCE survey design</strong>, <strong>study participants</strong>, <strong>statistical analyses</strong>

<br>

<strong>Marginal Rates of Substitution</strong>: These portray the rates at which respondents are willing to trade their choices in one attribute for preferred levels on another attribute

<br>

All in all, throughout the paper, the research question and the methods used to answer the question remain lucid, and the results are a cogent story that can give insights into the nature of the issue and provides a normative statement based on that story.

<br>

Right now, because I don't have a breadth or depth of knowledge in the field, I lack the foundational knowledge to judge papers as well/poorly written.

</p>
</div>
</div>

